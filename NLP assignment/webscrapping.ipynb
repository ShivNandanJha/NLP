{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article extracted and saved: extracted_articles/blackassign0001.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0002.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0003.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0004.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0005.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0006.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0007.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0008.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0009.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0010.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0011.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0012.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0013.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0014\n",
      "Article extracted and saved: extracted_articles/blackassign0015.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0016.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0017.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0018.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0019.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0020\n",
      "Article extracted and saved: extracted_articles/blackassign0021.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0022.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0023.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0024.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0025.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0026.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0027.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0028.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0029\n",
      "Article extracted and saved: extracted_articles/blackassign0030.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0031.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0032.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0033.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0034.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0035.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0036\n",
      "Article extracted and saved: extracted_articles/blackassign0037.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0038.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0039.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0040.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0041.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0042.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0043\n",
      "Article extracted and saved: extracted_articles/blackassign0044.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0045.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0046.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0047.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0048.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0049\n",
      "Article extracted and saved: extracted_articles/blackassign0050.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0051.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0052.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0053.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0054.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0055.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0056.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0057.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0058.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0059.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0060.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0061.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0062.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0063.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0064.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0065.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0066.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0067.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0068.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0069.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0070.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0071.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0072.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0073.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0074.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0075.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0076.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0077.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0078.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0079.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0080.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0081.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0082.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0083\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0084\n",
      "Article extracted and saved: extracted_articles/blackassign0085.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0086.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0087.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0088.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0089.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0090.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0091.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0092\n",
      "Article extracted and saved: extracted_articles/blackassign0093.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0094.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0095.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0096.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0097.txt\n",
      "Article extracted and saved: extracted_articles/blackassign0098.txt\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0099\n",
      "Error extracting article from {url}: {e}\n",
      "Article extraction failed for URL_ID: blackassign0100\n",
      "Extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"input.xlsx\")\n",
    "\n",
    "# Function to extract article text from URL\n",
    "\n",
    "\n",
    "def extract_article_text(URL):\n",
    "    try:\n",
    "\n",
    "        headers = {\n",
    "            'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "# Here the user agent is for Edge browser on windows 10. You can find your browser user agent from the above given link.\n",
    "        # Send a GET request to fetch the webpage content\n",
    "        response = requests.get(url=URL, headers=headers)\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the article title\n",
    "        title = soup.find('title').text.strip()\n",
    "        # Find the article text\n",
    "        article_text = soup.find('div', class_='td-post-content tagdiv-type').text.strip()\n",
    "        return title, article_text\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting article from {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Create a directory to save text files if it doesn't exist\n",
    "if not os.path.exists(\"extracted_articles\"):\n",
    "    os.makedirs(\"extracted_articles\")\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract URL and URL_ID from the DataFrame\n",
    "    URL = row['URL']\n",
    "    URL_ID = row['URL_ID']\n",
    "\n",
    "    # Extract article text from UR\n",
    "    title, article_text = extract_article_text(URL)\n",
    "\n",
    "    if title and article_text:\n",
    "        # Save the extracted article text to a text file\n",
    "        filename = f\"extracted_articles/{URL_ID}.txt\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(title + \"\\n\\n\")\n",
    "            file.write(article_text)\n",
    "        print(f\"Article extracted and saved: {filename}\")\n",
    "    else:\n",
    "        print(f\"Article extraction failed for URL_ID: {URL_ID}\")\n",
    "\n",
    "print(\"Extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words_path = './StopWords/'\n",
    "stop_words_files = os.listdir(stop_words_path)\n",
    "stop_words = set()\n",
    "for file in stop_words_files:\n",
    "    with open(os.path.join(stop_words_path, file), 'r') as f:\n",
    "        stop_words.update(f.read().splitlines())\n",
    "        \n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text, stop_words):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stop_words(text, stop_words):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative words\n",
    "positive_words_path = './MasterDictionary/positive-words.txt'\n",
    "negative_words_path = './MasterDictionary/negative-words.txt'\n",
    "with open(positive_words_path, 'r') as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "with open(negative_words_path, 'r') as f:\n",
    "    negative_words = set(f.read().splitlines())\n",
    "# Function to count positive and negative words\n",
    "def count_sentiment_words(text, positive_words, negative_words):\n",
    "    positive_count = sum(1 for word in text.split() if word in positive_words)\n",
    "    negative_count = sum(1 for word in text.split() if word in negative_words)\n",
    "    return positive_count, negative_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Process each file\n",
    "output_data = {'file_name': [], 'POSITIVE SCORE': [], 'NEGATIVE SCORE': []}\n",
    "articles_folder = './extracted_articles/'\n",
    "for file_name in os.listdir(articles_folder):\n",
    "    with open(os.path.join(articles_folder, file_name), 'r', encoding='utf-8') as f:\n",
    "        article_text = f.read()\n",
    "        # Remove stop words\n",
    "        clean_text = remove_stop_words(article_text, stop_words)\n",
    "        # Count positive and negative words\n",
    "        positive_count, negative_count = count_sentiment_words(clean_text, positive_words, negative_words)\n",
    "        output_data['file_name'].append(file_name)\n",
    "        output_data['POSITIVE SCORE'].append(positive_count)\n",
    "        output_data['NEGATIVE SCORE'].append(negative_count)\n",
    "\n",
    "# Write output to Excel\n",
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_excel('output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
